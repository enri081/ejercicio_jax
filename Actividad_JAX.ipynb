{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Actividad de investigación sobre JAX\n",
        "\n",
        "Enrique Moreno Alcántara\n",
        "\n",
        "Incluye:\n",
        "- Qué es JAX y características principales.\n",
        "- Comparación con TensorFlow y PyTorch.\n",
        "- Ecosistema alrededor de JAX.\n",
        "- Ejemplo práctico completo (optimización + autodiferenciación + `jit` + `vmap`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. ¿Qué es JAX? y características principales.\n",
        "\n",
        "JAX es una librería para computación numérica en Python (inspirada en NumPy) que destaca por:\n",
        "\n",
        "- **API tipo NumPy** (`jax.numpy`): código parecido a NumPy.\n",
        "- **Autodiferenciación** (reverse/forward-mode) mediante `jax.grad`, `jax.jacfwd`, `jax.jacrev`.\n",
        "- **Compilación con XLA** mediante `jax.jit` para acelerar operaciones.\n",
        "- **Vectorización automática** con `jax.vmap` (evita bucles Python).\n",
        "- **Paralelización** (multi-dispositivo) con `pmap` y `pjit`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1. jax.numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "x = jnp.array([1.0, 2.0, 3.0])\n",
        "y = jnp.sin(x) + x**2\n",
        "x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2. Autodiferenciación con `grad`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def f(theta):\n",
        "    return jnp.sum(jnp.sin(theta) + theta**2)\n",
        "\n",
        "grad_f = jax.grad(f)\n",
        "theta = jnp.array([0.1, 0.2, 0.3])\n",
        "print('f(theta)=', f(theta))\n",
        "print('grad f(theta)=', grad_f(theta))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3. `jit`: compilar para acelerar\n",
        "\n",
        "`jax.jit` compila la función con XLA. Suele dar gran mejora cuando hay operaciones repetidas.\n",
        "La primera llamada compila (puede tardar más); las siguientes suelen ser mucho más rápidas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def heavy(theta, n=2000):\n",
        "    # ejemplo artificial: muchas operaciones\n",
        "    z = theta\n",
        "    for _ in range(n):\n",
        "        z = jnp.tanh(z) + 0.01 * z\n",
        "    return jnp.sum(z)\n",
        "\n",
        "heavy_jit = jax.jit(heavy)\n",
        "\n",
        "theta = jnp.ones((2000,))\n",
        "\n",
        "t0 = time.time()\n",
        "out1 = heavy(theta)          # sin jit\n",
        "t1 = time.time()\n",
        "\n",
        "t2 = time.time()\n",
        "out2 = heavy_jit(theta)      # primera llamada: compila\n",
        "out2.block_until_ready()\n",
        "t3 = time.time()\n",
        "\n",
        "t4 = time.time()\n",
        "out3 = heavy_jit(theta)      # segunda llamada: ya compilado\n",
        "out3.block_until_ready()\n",
        "t5 = time.time()\n",
        "\n",
        "print('sin jit:', round(t1-t0, 4), 's')\n",
        "print('jit 1a:', round(t3-t2, 4), 's (incluye compilación)')\n",
        "print('jit 2a:', round(t5-t4, 4), 's')\n",
        "print('outputs iguales?', float(out1) == float(out3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.4. `vmap`: vectorización automática\n",
        "\n",
        "`vmap` permite aplicar una función sobre un batch sin escribir bucles Python.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def g(a, b):\n",
        "    return jnp.sum(a * b)\n",
        "\n",
        "# Queremos g para muchos pares (a_i, b_i)\n",
        "A = jnp.arange(12.0).reshape(4, 3)\n",
        "B = jnp.ones((4, 3))\n",
        "\n",
        "g_batched = jax.vmap(g, in_axes=(0, 0))\n",
        "print(g_batched(A, B))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Comparación: JAX vs TensorFlow vs PyTorch\n",
        "\n",
        "### 2.1. Estilo de programación\n",
        "- **JAX**: muy funcional; el rendimiento viene de `jit` + XLA; transformaciones (`grad`, `vmap`, `pmap`).\n",
        "- **PyTorch**: imperativo (eager) por defecto; excelente ergonomía para investigación; `torch.compile` intenta cerrar la brecha de compilación.\n",
        "- **TensorFlow**: tuvo histórico más orientado a gráficos; hoy es mayoritariamente eager con `tf.function` para compilar.\n",
        "\n",
        "### 2.2. Diferenciación automática\n",
        "- JAX: autodiff integrada, composable y con herramientas para jacobianos/hessianos.\n",
        "- PyTorch/TF: autodiff muy madura; ecosistemas grandes.\n",
        "\n",
        "### 2.3. Rendimiento y despliegue\n",
        "- JAX: muy fuerte en aceleradores y compilación XLA; popular en investigación a gran escala.\n",
        "- TF: ecosistema de producción amplio (TF Serving, TFLite).\n",
        "- PyTorch: fuerte en investigación; despliegue con TorchServe / exportadores; cada vez más completo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Ecosistema alrededor de JAX\n",
        "\n",
        "Algunas librerías populares:\n",
        "\n",
        "- **Flax**: redes neuronales estilo PyTorch/TF (muy usada en la comunidad JAX).\n",
        "- **Haiku** (DeepMind): alternativa para definir modelos.\n",
        "- **Optax**: optimizadores (Adam, SGD, etc.).\n",
        "- **Equinox**: enfoque *PyTorch-like* pero con transformaciones JAX.\n",
        "- **Diffrax**: ODE solvers diferenciables.\n",
        "- **BlackJAX**: MCMC / inferencia bayesiana.\n",
        "\n",
        "Integraciones típicas:\n",
        "- **XLA** (compilador), **CUDA**/**ROCm** (GPU), **TPU**.\n",
        "- Pipelines en Python con NumPy/pandas; visualización con matplotlib.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Ejemplo práctico: regresión lineal con grad + jit + vmap\n",
        "\n",
        "Vamos a implementar regresión lineal por descenso de gradiente en JAX.\n",
        "Objetivo: minimizar MSE sobre datos sintéticos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "key = jax.random.key(0)\n",
        "\n",
        "# Datos sintéticos: y = 3x + 2 + ruido\n",
        "n = 200\n",
        "x = jax.random.normal(key, (n, 1))\n",
        "true_w = jnp.array([[3.0]])\n",
        "true_b = jnp.array([2.0])\n",
        "noise = 0.3 * jax.random.normal(jax.random.key(1), (n, 1))\n",
        "y = x @ true_w + true_b + noise\n",
        "\n",
        "x.shape, y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict(params, x):\n",
        "    w, b = params\n",
        "    return x @ w + b  # (n,1)\n",
        "\n",
        "def mse_loss(params, x, y):\n",
        "    yhat = predict(params, x)\n",
        "    return jnp.mean((yhat - y) ** 2)\n",
        "\n",
        "loss_grad = jax.grad(mse_loss)\n",
        "\n",
        "# inicialización\n",
        "params = (jax.random.normal(jax.random.key(2), (1,1)) * 0.1,\n",
        "          jnp.zeros((1,)))\n",
        "\n",
        "print('loss inicial:', float(mse_loss(params, x, y)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def step(params, x, y, lr=0.1):\n",
        "    grads = loss_grad(params, x, y)\n",
        "    new_params = (params[0] - lr * grads[0],\n",
        "                  params[1] - lr * grads[1])\n",
        "    return new_params\n",
        "\n",
        "# Entrenamiento\n",
        "for epoch in range(200):\n",
        "    params = step(params, x, y, lr=0.2)\n",
        "\n",
        "print('loss final:', float(mse_loss(params, x, y)))\n",
        "print('w aprendido:', params[0].ravel())\n",
        "print('b aprendido:', params[1].ravel())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1. Evaluación en batch con `vmap`\n",
        "\n",
        "A veces queremos evaluar la loss de muchos parámetros (por ejemplo, en búsqueda aleatoria).\n",
        "`vmap` permite hacerlo de manera limpia.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creamos varios candidatos alrededor de params\n",
        "keys = jax.random.split(jax.random.key(3), 10)\n",
        "ws = params[0] + 0.2 * jax.random.normal(keys[0], (10,1,1))\n",
        "bs = params[1] + 0.2 * jax.random.normal(keys[1], (10,1))\n",
        "\n",
        "def loss_single(w, b):\n",
        "    return mse_loss((w,b), x, y)\n",
        "\n",
        "losses = jax.vmap(loss_single, in_axes=(0,0))(ws, bs)\n",
        "losses"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "entorno",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
